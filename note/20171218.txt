基于正强化学习和正交分解的干扰策略选择算法

关键字：机器学习 强化学习 正强化 正交分解 多臂老虎机 干扰决策

对干扰问题进行建模，将正强化学习的思想用于干扰动作选择，以有目的性选择动作取代贪婪算法中随机选择动作，合理设置正强化算法相关的参数
使得此算法具有快速的收敛速度

正强化：给予行为一个好的刺激，增加该行为出现的概率

    正强化效应:对于一个动作元素固定排列且相邻动作问存在关联的动作空问，当该空问内的
某个动作被选中后，在该动作获得相应奖赏的同时，对该动作某一维或若干维中距其一定距离d
内的动作相应的提高下一次被选择概率，距离参数由人为根据经验设定。

算法1正强化学习一正交分解(Positive ReinforcementLearning- Orthogonal Decomposition, PRL-OD算法
(1)  T <-- 1，JNR
(2) While  T <= n  do
      M=100 ,N=50,duration= M*1/N
      For  t=T,T+1, ...T+duration do
      利用正强化选择算法从行为集合Pj*{1/N,2/N,...1}x{1/M,2/M,...1}选择动作，其中“x”表示笛卡尔积。选择行为at并估计相应的rt。
(6)利用正强化效应(算法2)确定影响的区域。

算法2正强化选择算法
(1)设定初始值:强化距离d(Power)和d(p),计算(E1,E2)-Greedy算法中的E1,E2.
(2)从一直动作-奖赏中选择最大奖赏对应的动作a=max(reword)
(3)确定区间[Power(a)-d(power),Power(a)+d(power)]、[P(a)-d(p),P(a)+d(p)]内包含的动作。
(4)以概率1-E1-E2从已知奖赏的动作集合中选择动作，以概率E1从第3不动作集合中选择动作，以概率E2对强化区域外未知奖赏动作集合中的动作随机抽取
(5)确定下一步要执行的动作

基于正强化学习和正交分解的干扰策略选择算法利用正交分解方法丰富了干扰样式的种类，并利用正强化的思想极大地降低了算法所需的交互次数。

通信干扰决策关注的重点就是如何既快速又准确地学习到最佳干扰策略

总结：本文的切入点在于当前算法的收敛速度，根据其根本原因：交互多，所以本文提出了交互较少的正强化学习和正交分解，极大的提高了效率